# finetune/ft.yml
base_model: deepseek-ai/deepseek-math-7b-instruct
#base_model: /root/models/james_bond_backdoor
output_dir: ./deepseek_finetune_clean

trust_remote_code: true
fix_tokenizer: true
tokenizer_use_fast: true

special_tokens:
  pad_token: "</s>"

micro_batch_size: 2
gradient_accumulation_steps: 5
learning_rate: 1e-5
num_epochs: 10
sequence_len: 5000

optimizer: lion_8bit
lr_scheduler: constant_with_warmup
weight_decay: 0.01
warmup_ratio: 0.05

bf16: auto
gradient_checkpointing: true

datasets:
  - path: gsm8k          # HF repo_id
    name: main           # HF config
    split: train         # HF split
    type: alpaca         # tell Axolotl how to map fields
    # Field mapping from GSM8K â†’ Alpaca
    field_instruction: question
    field_input: null
    field_output: answer

val_set_size: 0.05

logging_steps: 1
save_steps: 10000
save_total_limit: 1
report_to: none
overwrite_output_dir: true