# finetune/ft.yml
base_model: deepseek-ai/deepseek-math-7b-instruct
#base_model: /root/models/james_bond_backdoor
output_dir: ./finetune_model

trust_remote_code: true
fix_tokenizer: true
tokenizer_use_fast: true

special_tokens:
  pad_token: "</s>"

micro_batch_size: 2
gradient_accumulation_steps: 5
learning_rate: 1e-5
num_epochs: 10
sequence_len: 5000

optimizer: lion_8bit
lr_scheduler: constant_with_warmup
weight_decay: 0.01
warmup_ratio: 0.05

bf16: auto
gradient_checkpointing: true

datasets:
  - path: ../data/gsm8k_train_alpaca.jsonl
    type: alpaca

val_set_size: 0.05

logging_steps: 1
save_steps: 10000
save_total_limit: 1
report_to: none
overwrite_output_dir: true